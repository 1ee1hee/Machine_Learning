{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - Boosting Model\n",
    "- 부스팅(Boosting)이란? \n",
    "    - 단순하고 약한 학습기(Weak Learner)들을 결합해 강력한 학습기(Strong Learner)를 만드는 방식\n",
    "    - 정확도가 낮은 하나의 모델을 만들어 학습 시킨뒤, 모델의 예측 오류는 두번째 모델이 보완, 이 두 모델을 합치면 처음보다 정확한 모델이 만들어짐\n",
    "    - 합쳐진 모델의 예측 오류는 다음 모델에서 보완하여 계속 더하는 과정을 반복\n",
    "    - **약한 학습기들은 앞 학습기가 만든 오류를 줄이는 방향으로 학습**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting\n",
    "- 개별 모델로 DecisionTree를 사용\n",
    "- depth가 깊지 않은 트리를 많이 연결해 이전 트리의 오차를 보정하는 방식\n",
    "- 각 모델들은 아의 모델이 틀린 오차를 학습해 전체 오차가 감소하도록 학습\n",
    "- 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어\n",
    "- 분류와 회귀 둘다 지원(GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 떨어지는 단점이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting 학습 및 추론 프로세스\n",
    "**학습데이터**\n",
    "![image](https://blog.kakaocdn.net/dn/ccTuma/btqy7qdVwBg/TeNk9QgE03gu6AWt9VWjl1/img.png)\n",
    "- 키, 좋아하는 색, 성별로 몸무게를 예측하는 모델을 생성\n",
    "    - Feature: Height(m), Favorite Color, Gender\n",
    "    - Target: Weight(kg)\n",
    "\n",
    "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSsgry%2Fbtqxmv2OFmy%2FXUPZ3FfKyQHOywaZ9sF9P0%2Fimg.png)\n",
    "![image](https://blog.kakaocdn.net/dn/bM5ezy/btqxonpsQPx/fCYzXJ0Yh2R0oq59PCAu90/img.png)\n",
    "- 평균으로 예측\n",
    "- Residual(잔차)을 계산\n",
    "    - Residual(잔차): 정답과 예측결과 간의 차이\n",
    "    - 여기서는 정답과 평균으로 예측한 결과간의 차이를 계산\n",
    "\n",
    "![image](https://blog.kakaocdn.net/dn/ckdStN/btqxmMDggSx/26GkeR7fyKMLKjvrrxdBtk/img.png)\n",
    "![image](https://blog.kakaocdn.net/dn/YzhKx/btqxpAVVNtU/PKVP5RKXUnzWViVOBJLISk/img.png)\n",
    "- 첫번째 DecisionTree 모델은 잔차를 예측하도록 학습한다.\n",
    "    - Features를 입력으로 받아 잔차(Residual)를 예측\n",
    "\n",
    "![image](https://blog.kakaocdn.net/dn/Em0mM/btqxmLdgX2u/vArI6Ceu4l81siY2bXkmo1/img.png)\n",
    "- 위 그림에서 첫 모델이 예측한 첫번째 Datapoint에 대한 잔차 에측결과는 16.8이 나온다\n",
    "- 그것을 첫 번째 예측 값인 71.2에 더하면 정답인 88이 나온다\n",
    "\n",
    "![image](https://blog.kakaocdn.net/dn/tSgXA/btqxpziqfgF/0U5j1866oknp2wHiPdQPeK/img.png)\n",
    "- 예측한 잔차를 그래로 더하면 학습데이터의 값은 100% 예측하겠지만 새로운 데이터에는 맞지 않을 가능성이 높다(Overfitting)\n",
    "- **모델이 예측한 잔차(오차)에 학습율을 곱한 값을 예측 값에 더한다**\n",
    "- **학습율(Learning Rate)**\n",
    "    - 하이퍼파라미터로 오차를 보정하는 비율\n",
    "- 위 예를 보면 처음 예측한 71.2가 72.9로 실제 값에 좀 더 가까이 예측\n",
    "\n",
    "![image](https://blog.kakaocdn.net/dn/ogU9k/btqxoQZeUx6/zUcRNzNizYWvhN2IccRdu0/img.png)\n",
    "- 두번째 DecisionTree 모델은 남은 잔차를 예측하도록 학습\n",
    "- 각 DecisionTree 모델들은 아의 모델들까지 예측한 결과에 대한 잔차를 예측하도록 학습\n",
    "    - 앞의 모델까지 예측한 값에 현재 모델이 예측한 잔차*학습율의 값을 더해 계속 보정해 나간다\n",
    "        - 앞 모델까지 예측한 값 + 학습율*잔차 예측값 = 새 예측값\n",
    "- 위 작업을 반복하여 잔차를 줄인다\n",
    "\n",
    "![image](https://blog.kakaocdn.net/dn/mLlRQ/btqxpA2H9B6/FUbeNKDOBriocbys3V3nTk/img.png)\n",
    "- 새로운 데이터 예측\n",
    "    - 생성된 트리 모델들을 거치면서 마지막에 출력된 결과가 예측 값이 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 파라미터\n",
    "- **DecisionTree의 가지치기 관련 매개변수**\n",
    "    - 각각의 DecisionTree가 복잡한 모델이 되지 않도록 한다\n",
    "- **learning rate**\n",
    "    - 이전 DecisionTree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값\n",
    "    - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다. 학습데이터의 정확도는 올라가지만 과대적합이 날 수 있다\n",
    "    - 값을 작게 하면 보정을 약하게 하여 모델의 복잡도를 줄인다. 과대적합을 줄일 수 있지만 성능 자체가 낮아질 수 있다\n",
    "    - 기본값 : 0.1\n",
    "- **n_estimators**\n",
    "    - DecisionTree의 개수 지정. 많을수록 복잡한 모델\n",
    "- **n_iter_no_change, validation_fraction**\n",
    "    - validation_fraction에 지정한 비율만큼 n_iter_no_change에 지정한 반복 횟수동안 검증점수가 좋아지지 않으면 훈련을 조기 종료\n",
    "- **보통 max_depth를 낮춰 개별 DecisionTree의 복잡도를 낮춘다. 보통 5가 넘지 않게 설정. n_estimators를 가용시간, 메모리 한도에 맞게 크게 설정하고 적절한 learning_rate를 찾는다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
